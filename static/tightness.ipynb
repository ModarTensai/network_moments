{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Network Moments (GNMs)\n",
    "\n",
    "Let $\\mathbf{x}\\sim\\mathcal{N}\\left(\\mathbf{\\mu}, \\mathbf{\\Sigma}\\right)$ and $q(x) = \\max(x, 0)$ where $\\Phi(x)$ and $\\varphi(x)$ are the CDF and PDF of the normal distribution,\n",
    "\n",
    "$\\mathbb{E}\\left[q(\\mathbf{x})\\right] = \\mathbf{\\mu}\\odot\\Phi\\left(\\mathbf{\\mu}\\oslash\\mathbf{\\sigma}\\right) + \\mathbf{\\sigma}\\odot\\varphi\\left(\\mathbf{\\mu}\\oslash\\mathbf{\\sigma}\\right)$\n",
    "where $\\mathbf{\\sigma} = \\sqrt{\\text{diag}\\left(\\mathbf{\\Sigma}\\right)}$ with\n",
    "$\\odot$ and $\\oslash$ as element-wise product and division.\n",
    "\n",
    "$\\mathbb{E}\\left[q^2(\\mathbf{x})\\right] = \n",
    "\\left(\\mathbf{\\mu}^2+\\mathbf{\\sigma}^2\\right) \\odot \\Phi\\left(\\mathbf{\\mu}\\oslash\\mathbf{\\sigma}\\right) + \\mathbf{\\mu} \\odot \\mathbf{\\sigma} \\odot \\varphi\\left(\\mathbf{\\mu}\\oslash\\mathbf{\\sigma}\\right)$\n",
    "where $\\text{var}\\left[q(\\mathbf{x})\\right] = \\mathbb{E}\\left[q^2(\\mathbf{x})\\right] - \\mathbb{E}\\left[q(\\mathbf{x})\\right]^2$\n",
    "\n",
    "$\\left.\\mathbb{E}\\left[q(\\mathbf{x})q(\\mathbf{x})^\\top\\right]\\right|_{\\mathbf{\\mu} = \\mathbf{0}} = c\\left(\\mathbf{\\Sigma}\\oslash\\mathbf{\\sigma}\\mathbf{\\sigma}^\\top\\right) \\odot \\mathbf{\\sigma}\\mathbf{\\sigma}^\\top$\n",
    "where $c(x) = \\frac{1}{2\\pi}\\left(x\\cos^{-1}(-x)+\\sqrt{1-x^2}\\right)$\n",
    "(Note: $\\left|c(x) - \\Phi(x - 1)\\right| < 0.0241$)\n",
    "\n",
    "$\\text{cov}\\left[q(\\mathbf{x})\\right] = \\mathbb{E}\\left[q(\\mathbf{x})q(\\mathbf{x})^\\top\\right] - \\mathbb{E}\\left[q(\\mathbf{x})\\right]\\mathbb{E}\\left[q(\\mathbf{x})\\right]^\\top$\n",
    "where $\\left.\\text{cov}\\left[q(\\mathbf{x})\\right]\\right|_{\\mathbf{\\mu} = \\mathbf{0}} = \\left.\\mathbb{E}\\left[q(\\mathbf{x})q(\\mathbf{x})^\\top\\right]\\right|_{\\mathbf{\\mu} = \\mathbf{0}} - \\frac{1}{2\\pi}\\mathbf{\\sigma}\\mathbf{\\sigma}^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Moments (NMs) [PyTorch Implementation].\n",
      "\n",
      "Let x be a random variable with some mean M and covariance S.\n",
      "x can be multivariate of size (N), so S of size (N, N) and M of size (N).\n",
      "S = C - outer_product(M, M), where C is the correlation matrix.\n",
      "M and C are the expectations of x and outer_product(x, x), respectively.\n",
      "The n-th moment of x is the expectation of x_to_the_power_n.\n",
      "The diagonal of S and C are the variance and second_moment, respectively.\n",
      "The second_moment is the expectation of x_squared.\n",
      "The variance = second_moment - M_squared.\n",
      "\n",
      "For any function acting on x (e.g., f(x)),\n",
      "we want to compute its probability density function (i.e., of f(x)).\n",
      "A simpler task maybe is to find the n-th-moment of the function for all n > 0.\n",
      "\n",
      "This package is trying to find closed form expressions for the output\n",
      "probabilistic moments of some functions given some input distributions.\n",
      "\n",
      "Network Moments Structure:\n",
      "[{'affine': ['batch_moments', 'covariance', 'mean', 'variance']},\n",
      " {'affine_relu_affine': ['mean', 'special_covariance', 'special_variance']},\n",
      " {'net': ['AlexNet',\n",
      "          'Classifier',\n",
      "          'LeNet',\n",
      "          'Sequential',\n",
      "          {'adf': ['gaussian']}]},\n",
      " {'relu': ['batch_moments',\n",
      "           'mean',\n",
      "           'moments',\n",
      "           'variance',\n",
      "           'zero_mean_covariance']},\n",
      " {'utils': ['Flatten',\n",
      "            'MatrixSquareRoot',\n",
      "            'cov',\n",
      "            'epsilon',\n",
      "            'even_zip',\n",
      "            'flatten',\n",
      "            'jac_at_x',\n",
      "            'jacobian',\n",
      "            'lin_at_x',\n",
      "            'linearize',\n",
      "            'map_batch',\n",
      "            'mul_diag',\n",
      "            'normalize',\n",
      "            'normalize_',\n",
      "            'outer',\n",
      "            {'rand': ['definite', 'from_eigen']},\n",
      "            'special_sylvester',\n",
      "            'sqrtm',\n",
      "            {'stats': ['cov',\n",
      "                       'estimate_pdf',\n",
      "                       {'gaussian': ['density',\n",
      "                                     'fit',\n",
      "                                     'gaussianity',\n",
      "                                     'hypothesis_test',\n",
      "                                     'normal_density']},\n",
      "                       'hist_as_func',\n",
      "                       'integrate_area',\n",
      "                       'inter_quartile_range',\n",
      "                       'kurtosis_score',\n",
      "                       'normalized_histogram',\n",
      "                       'num_hist_bins',\n",
      "                       'pdf_similarity',\n",
      "                       'percentile',\n",
      "                       'skewness_score']},\n",
      "            'triu_numel',\n",
      "            'verbosify']}]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import unittest\n",
    "from torch import nn\n",
    "from pprint import pprint\n",
    "from types import ModuleType\n",
    "import network_moments.torch as nm\n",
    "\n",
    "seed = 77  # for reproducability\n",
    "\n",
    "def traverse(obj, exclude=[]):\n",
    "    data = []\n",
    "    if type(obj) is not ModuleType:\n",
    "        return data\n",
    "    for e in dir(obj):\n",
    "        if not e.startswith('_') and all(e != s for s in exclude):\n",
    "            sub = traverse(obj.__dict__[e], exclude)\n",
    "            data.append(e if len(sub) == 0 else {e:sub})\n",
    "    return data\n",
    "\n",
    "print(nm.__doc__)\n",
    "print('Network Moments Structure:')\n",
    "pprint(traverse(nm.gaussian, exclude=['tests', 'general']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tightness of the expressions on using tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_covariance (network_moments.torch.general.affine.tests.tests.Batch) ... ok\n",
      "test_diagonal_covariance (network_moments.torch.general.affine.tests.tests.Batch) ... ok\n",
      "test_diagonal_variance (network_moments.torch.general.affine.tests.tests.Batch) ... ok\n",
      "test_mean (network_moments.torch.general.affine.tests.tests.Batch) ... ok\n",
      "test_variance (network_moments.torch.general.affine.tests.tests.Batch) ... ok\n",
      "test_covariance (network_moments.torch.general.affine.tests.tests.Tightness) ... ok\n",
      "test_mean (network_moments.torch.general.affine.tests.tests.Tightness) ... ok\n",
      "test_variance (network_moments.torch.general.affine.tests.tests.Tightness) ... ok\n",
      "test_mean_batch_mean (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_mean_batch_mean_and_variance (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_mean_batch_variance (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_variance_batch_mean (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_variance_batch_mean_and_variance (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_variance_batch_variance (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_zero_mean_covariance (network_moments.torch.gaussian.relu.tests.tests.Batch) ... ok\n",
      "test_mean (network_moments.torch.gaussian.relu.tests.tests.Tightness) ... ok\n",
      "test_variance (network_moments.torch.gaussian.relu.tests.tests.Tightness) ... ok\n",
      "test_zero_mean_covariance (network_moments.torch.gaussian.relu.tests.tests.Tightness) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 18 tests in 5.950s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "runner = unittest.TextTestRunner(sys.stdout, verbosity=2)\n",
    "load = unittest.TestLoader().loadTestsFromModule\n",
    "result = runner.run(unittest.TestSuite([\n",
    "    load(nm.gaussian.affine.tests),\n",
    "    load(nm.gaussian.relu.tests),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the tightness of the expressions on affine-ReLU-affine networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output variance of Affine-ReLU-Affine for special Gaussian input.\n",
      "\n",
      "    f(x) = B*max(A*x+c1, 0)+c2, where c1 = -A*input_mean.\n",
      "\n",
      "    For this specific c1, this function doesn't depend on\n",
      "    neither the input mean nor the biases.\n",
      "\n",
      "    Args:\n",
      "        covariance: Input covariance matrix (Batch, Size, Size)\n",
      "            or variance vector (Batch, Size) for diagonal covariance.\n",
      "        A: The A matrix (M, Size).\n",
      "        B: The B matrix (N, M).\n",
      "        variance: Whether the input covariance is a diagonal matrix.\n",
      "        stability: For accurate results this should be zero\n",
      "            if used in training, use a value like 1e-4 for stability.\n",
      "\n",
      "    Returns:\n",
      "        Output variance of Affine-ReLU-Affine for Gaussian input\n",
      "        with mean = `mean` and covariance matrix = `covariance`\n",
      "        where the bias of the first affine = -A*`mean`.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "rand = nm.utils.rand\n",
    "gnm = nm.gaussian.affine_relu_affine\n",
    "print(gnm.special_variance.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.97120821 1.00030876 0.99930142]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[1.00029394 1.00071615 1.00040951]\n"
     ]
    }
   ],
   "source": [
    "length = 3\n",
    "count = 1000000\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# input mean and covariance\n",
    "mu = torch.randn(length, dtype=dtype, device=device)\n",
    "cov = rand.definite(length, dtype=dtype, device=device,\n",
    "                    positive=True, semi=False, norm=1.0)\n",
    "\n",
    "# variables\n",
    "A = torch.randn(length, length, dtype=dtype, device=device)\n",
    "c1 = -A.matmul(mu)  # torch.randn(length, dtype=dtype)\n",
    "B = torch.randn(length, length, dtype=dtype, device=device)\n",
    "c2 = torch.randn(length, dtype=dtype, device=device)\n",
    "\n",
    "# analytical output mean and variance\n",
    "out_mu = gnm.mean(mu, cov, A, c1, B, c2)\n",
    "out_var = gnm.special_variance(cov, A, B)\n",
    "\n",
    "# Monte-Carlo estimation of the output mean and variance\n",
    "normal = torch.distributions.MultivariateNormal(mu, cov)\n",
    "samples = normal.sample((count,))\n",
    "out_samples = samples.matmul(A.t()) + c1\n",
    "out_samples = torch.max(out_samples, torch.zeros([], dtype=dtype, device=device))\n",
    "out_samples = out_samples.matmul(B.t()) + c2\n",
    "mc_mu = torch.mean(out_samples, dim=0)\n",
    "mc_var = torch.var(out_samples, dim=0)\n",
    "\n",
    "# printing the ratios\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tightness of A (best is zero): 0.0\n",
      "Tightness of b (best is zero): 0.0\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "num_classes = 10\n",
    "image_size = (28, 28)\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "\n",
    "size = torch.prod(torch.tensor(image_size)).item()\n",
    "x = torch.rand(batch, *image_size, dtype=dtype, device=device)\n",
    "model = nn.Sequential(\n",
    "    nm.utils.flatten,\n",
    "    nn.Linear(size, num_classes),\n",
    ")\n",
    "model.type(dtype)\n",
    "if device.type != 'cpu':\n",
    "    model.cuda(device.index)\n",
    "\n",
    "jac, bias = nm.utils.linearize(model, x)\n",
    "A = list(model.children())[1].weight\n",
    "print('Tightness of A (best is zero): {}'.format(\n",
    "    torch.max(torch.abs(jac - A)).item()))\n",
    "\n",
    "b = list(model.children())[1].bias\n",
    "print('Tightness of b (best is zero): {}'.format(\n",
    "    torch.max(torch.abs(bias - b)).item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-stage linearization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte-Carlo mean / Analytical mean:\n",
      "[0.99435257 1.00150285 1.00178643 0.99918984 1.00057891 1.00208202\n",
      " 0.99765424 1.00304414 1.00054557 1.01053102]\n",
      "Monte-Carlo variance / Analytical variance:\n",
      "[1.3365327  1.14314124 1.33544767 1.4502823  1.53299597 1.52962128\n",
      " 1.1255225  1.21295774 1.70099599 1.49284563]\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "num_classes = 10\n",
    "image_size = (28, 28)\n",
    "dtype = torch.float64\n",
    "device = torch.device('cpu', 0)\n",
    "gnm = nm.gaussian.affine_relu_affine\n",
    "\n",
    "size = torch.prod(torch.tensor(image_size)).item()\n",
    "x = torch.rand(1, *image_size, dtype=dtype, device=device)\n",
    "\n",
    "# deep model\n",
    "first_part = nn.Sequential(\n",
    "    nm.utils.flatten,\n",
    "    nn.Linear(size, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 300),\n",
    ")\n",
    "first_part.type(dtype)\n",
    "relu = nn.Sequential(\n",
    "    nn.ReLU(),\n",
    ")\n",
    "relu.type(dtype)\n",
    "second_part = nn.Sequential(\n",
    "    nn.Linear(300, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes),\n",
    ")\n",
    "second_part.type(dtype)\n",
    "if device.type != 'cpu':\n",
    "    first_part.cuda(device.index)\n",
    "    relu.cuda(device.index)\n",
    "    second_part.cuda(device.index)\n",
    "def model(x):\n",
    "    return second_part(relu(first_part(x)))\n",
    "\n",
    "# variables\n",
    "A, c1 = nm.utils.linearize(first_part, x)\n",
    "B, c2 = nm.utils.linearize(second_part, relu(first_part(x)).detach())\n",
    "x.requires_grad_(False)\n",
    "A.squeeze_()\n",
    "c1.squeeze_()\n",
    "B.squeeze_()\n",
    "c2.squeeze_()\n",
    "\n",
    "# analytical output mean and variance\n",
    "mean = x.view(-1)\n",
    "covariance = rand.definite(size, norm=0.1, dtype=dtype, device=device)\n",
    "out_mu = gnm.mean(mean, covariance, A, c1, B, c2)\n",
    "out_var = gnm.special_variance(covariance, A, B)\n",
    "\n",
    "# Monte-Carlo estimation of the output mean and variance\n",
    "normal = torch.distributions.MultivariateNormal(mean, covariance)\n",
    "samples = normal.sample((count,))\n",
    "out_samples = model(samples.view(-1, *image_size)).detach()\n",
    "mc_mu = torch.mean(out_samples, dim=0)\n",
    "mc_var = torch.var(out_samples, dim=0)\n",
    "\n",
    "# printing the ratios\n",
    "print('Monte-Carlo mean / Analytical mean:')\n",
    "print((mc_mu / out_mu).cpu().numpy())\n",
    "print('Monte-Carlo variance / Analytical variance:')\n",
    "print((mc_var / out_var).cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
